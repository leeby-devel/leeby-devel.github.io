[ { "title": "JEP 346 &gt; unused (미사용) committed memory 를 OS 로 반환 (feat. 인프라 비용 절감)", "url": "/posts/JEP346-unused-committed-memory-%EB%A5%BC-os-%EB%A1%9C-%EB%B0%98%ED%99%98/", "categories": "JVM, GC", "tags": "JEP, JVM, JDK, G1GC, GC", "date": "2024-11-10 21:39:00 +0900", "snippet": "들어가며…최근 회사에서 JDK, Spring Boot, Kotlin, Gradle 등 기반 시스템 버전 업그레이드하는 작업을 주도했다.그 중 JDK 버전은 11 → 17, 17 → 21 두 단계로 진행할 계획이었고, 계획대로 JDK 17 부터 버전 업데이트를 했다.업데이트 후 애플리케이션 메트릭을 살펴보는데 Heap 메모리 풋프린트가 이전과 다른 양상을 보였고, 이번 포스트에서 해당 내용을 다뤄보고자 한다.JVM Heap 메모리먼저 JVM 힙메모리 상태에 대한 용어부터 짚고 가는 게 좋을 것 같다. Grafana JVM Micrometer 기준으로 JVM 힙 메모리는 used, committed, max (= reserved) 로 나뉜다.committed == max 인 상태 used: 애플리케이션 힙의 라이브 오브젝트들에 의해 점유되고 있는 메모리이다. RSS (of heap): 애플리케이션이 물리적으로 (실제로) 점유하고 있는 메모리 영역이다. Grafana 에서 표현되지 않으므로 대충 가상의 선을 그었다. committed 영역에서 실제로 사용중인 부분으로 이해하면 된다. pmap 으로 관찰 가능하다. committed: 애플리케이션이 OS 로 부터 미리 할당받아 놓은 메모리 영역이다. 실제로 사용(RSS)되고 있을 수도 있고 아닐 수도 있다. reserved 까지 확장될 수 있다. reserved (= max): 애플리케이션이 OS 로부터 할당받을 수 있는 메모리의 최대 크기이다. ref. https://stackoverflow.com/a/31178912기존 (JDK 11) 까지의 메모리 설정JDK 11 까지는 InitialRAMPercentage 를 MaxRAMPercentage 와 같게 두거나, xms 값을 xmx 값과 같게 두는 것이 권장되었다.그 이유는 다음과 같은 오버헤드를 피하기 위해서였다.committed 크기가 작다.→ GC 주기가 짧아지고 이는 오버헤드로 작용한다.committed 크기가 reserved 와 달라 필요에 따라 reserved 까지 점진적으로 크기가 확장된다.→ OS 로부터 메모리를 할당받는 과정에서 오버헤드가 발생한다.👉 부연 설명(G1GC 기준) GC Cycle 은 committed 메모리 범위 내에서 수행된다. 따라서 committed 메모리 크기가 작으면 그 만큼 GC 주기가 짧아진다.엄밀하게는 GC Cycle 은 사용중인 메모리 (RSS) 범위 내에서 수행되고 추가 메모리가 필요하면 RSS 가 committed 만큼 확장되는 것이다.이후에 committed 메모리로도 부족하면 OS 에 추가 메모리를 요청하고, 이 크기는 reserved 만큼 늘어날 수 있다.중요한 점은, JDK 11 까지는 한번 committed 된 메모리는 (특별한 경우가 아니면) OS 로 반환되지 않는다는 점이다.예를 들어, 위 설정과 다르게 InitialRAMPercentage 값을 낮게 설정했더라도 트래픽이 많은 시간대에서 확장된 메모리 크기는 트래픽이 적은 새벽이 된다고 해서 줄어들지 않는다.즉 앱이 에이징 됨에 따라 committed 메모리는 reserved 까지 계속해서 증가하는 양상을 띄게 된다. (트래픽이 있는 애플리케이션이라면)JEP 346 &amp;gt; 메모리 관리 패턴 개선 사항이러한 메모리 사용 패턴은 사용량만큼 비용을 지불하는 클라우드 환경에서는 큰 단점으로 작용한다.JEP 346 에서는 이러한 점을 개선하기 위해, 유휴 committed 메모리는 OS 로 반환되도록 G1GC 의 메모리 사용 패턴에 변화를 줬다.위 내용은 “JEP 346 &amp;gt; Motivation” 에 잘 나와있다.JEP 346 개선사항은 JDK 12 부터 적용되었기 때문에 JDK 11 → 17 업데이트 이후에 현상을 발견한 것이었다.개선 전후 메트릭 비교다음은 JDK 버전 외에 JVM 설정 값 등 모든 환경이 같은 동일 애플리케이션 메트릭이다.JDK &amp;lt;= 11JDK &amp;gt;= 12 → 미사용 committed 메모리가 OS 로 반환되는 모습을 볼 수 있다.두 환경 모두 InitialRAMPercentage, MaxRAMPercentage 두 값을 동일하게 75% 로 두었다.눈여겨 볼 점은 JDK &amp;gt;= 12 에서는 InitialRAMPercentage 옵션이 기대와 같이 동작하지 않는 다는 점이다.예상 대로면 committed, reserved (= max) 풋프린트가 같아야 하기 때문이다.InitialRAMPercentage 사용성 이해 JEP 346 &amp;gt; Risks and Assumptions When this feature (이번 개선 사항) is enabled, the VM runs these periodic collections under the conditions above regardless of other options. E.g. the VM could make an assumption that if the user sets -Xms to -Xmx and other (combinations of) options to get minimal and consistent garbage collection pauses. This will not be the case for consistency reasons.이는 JEP 346 에서 메모리 관리 기능이 변경되었고 이 기능이 우선되어 InitialRAMPercentage 를 아무리 높게 설정하여도 실제 사용되지 않는 메모리는 committed 풋프린트로 잡히지 않게 된 것이다.따라서 JDK 12 이후부터는 InitialRAMPercentage 값을 MaxRAMPercentage 값과 같게 (= 높게) 두는 것은 아무 의미가 없어졌다.다만 InitialRAMPercentage 옵션은 여전히 초기 힙 메모리를 설정하는 옵션으로서 의미가 있다.JEP 346 개선사항으로 인해 InitialRAMPercentage 옵션이 직접적으로 변경된 것은 아니지만 간접적인 영향을 받았고, 옵션의 사용 맥락에 있어 약간의 변화가 생겼다고 이해하면 될 것 같다.필요할 때만 메모리를 확장하고 필요없을 때는 OS 로 메모리를 반환하는 패턴은 결과적으로 Worker Node 관점에서의 메모리 풋프린트를 크게 줄여줬다. (애플리케이션에 따라 대략 60% ~ 30% 정도)JEP 346 개선사항은 클라우드 환경이 너무 당연한 요즘, 인프라 비용을 절감할 수 있는 아주 매력적인 피쳐라는 생각이 든다." }, { "title": "k8s 환경에서 패킷 캡쳐하는 방법 &gt; tcpdump, wireshark", "url": "/posts/k8s-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%8C%A8%ED%82%B7-%EC%BA%A1%EC%B3%90%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95/", "categories": "Network", "tags": "tcpdump, wireshark, k8s", "date": "2024-10-28 23:52:00 +0900", "snippet": "k8s 환경에서 서버를 운용하다보면 종종 파드에서 발생한 네트웍 이슈 트러블 슈팅을 위해 패킷 단위 분석이 필요한 경우가 있다.이 때 tcpdump 로 생성한 덤프 파일을 시각화하기 위해 wireshark 를 활용한다. tcpdump 외에도 ksniff 같은 더 편리한 도구도 있다고 한다. 나중에 사용해 봐야겠다!준비물1. pod 에 tcpdump 설치2. 로컬에 wireshark 설치요약1. pod &amp;gt; tcpdump 로 .pcap 파일 생성2. 생성된 .pcap 파일을 kubectl cp 커맨드로 로컬에 복사3. .pcap 파일을 wireshark 로 분석1. 대상 pod 에서 덤프 파일 생성$ tcpdump -i any -A -vvv -nn host &amp;lt;Destination IP&amp;gt; -w target_dump_file.pcapIP 를 특정할 필요가 없으면 적지 않아도 된다.tcpdump 를 실행하면 패킷 덤프가 시작되는데, Ctrl + C 로 중단하면 .pcap 파일이 생성된다.네트웍 이슈가 발생한 시점에 중단하여 .pcap 파일을 생성한다.tcpdump 옵션 플래그 설명 -nn 출력 결과의 포트를 변환 없이 그대로 출력한다. (e.g. http - 80, https - 443) -A 패킷의 내용을 ACSII 형태로 출력한다. 출처. tcpdump 완전정복하기[Part.1]2. pod -&amp;gt; local 덤프 파일 복사pod 에서 local 로 파일을 복사하기 위해 kubectl 을 활용한다.$ kubectl cp &amp;lt;대상 pod&amp;gt;:&amp;lt;경로&amp;gt;/&amp;lt;파일&amp;gt; &amp;lt;로컬 경로&amp;gt;/&amp;lt;복사할 파일명&amp;gt;3. wireshark 로 .pcap 파일 분석wireshark 를 실행한 뒤 .pcap 파일을 드래그 앤 드랍하면 패킷 내용이 보기 좋게 출력된다." }, { "title": "JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc", "url": "/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/", "categories": "JVM", "tags": "JDK, JVM, leak, C, glibc, malloc, jcmd", "date": "2024-07-18 20:54:00 +0900", "snippet": "몇 개월 전에 회사 애플리케이션에서 발생한 메모리 누수를 트러블 슈팅 &amp;amp; 픽스한 내용을 포스트로 다루었다. 누수를 잡아낸 후에도 미량이지만 애플리케이션 전체 메모리 사용량 (RSS 기준)이 계속해서 늘어나고 있었다.원인을 파헤쳐보니 크리티컬한 내용은 아니나, GNU C 라이브러리 (glibc) 위에 구동되는 JVM 은 알게 모르게 영향을 받을 내용이라서 이곳에 그 내용을 공유해본다. 글의 내용이 glibc 를 사용하는 시스템을 전제로 하기 때문에 이 부분을 먼저 확인하시면 좋을 것 같습니다.대표적으로 많이 사용하는 OS, JDK 벤더 기준으로 표를 구성해봤다.잡담adoptopenjdk/alpine 의 경우 alpine 의 기본 C 라이브러리인 musl 의 안정성 검토가 다 되지 않아JDK 15 이전까지는 glibc 를 별도로 설치해서 사용해왔다.JDK 16 부터는 musl 을 사용한다고 하는데, 호환성이 좋을지는 모르겠다. 향후 파트 내 시스템을 JDK 21 로 마이그레이션할 계획인데 이땐 alpine 대신 debian-slim 을 사용해야겠다.끝나지 않은 메모리 증가 현상메모리 누수를 픽스한 이후 수 일간의 파드 모니터링 과정에서 파드가 죽고있거나 메모리 사용량이 눈에 띄게 증가하진 않았다. 하지만 왜인지 모를 이유로 메모리 사용량이 미량이지만 아주 꾸준하게 늘어나고 있다.도대체 뭘까?Heap, Non-Heap, Direct Buffer 확인우선 메모리의 어떤 영역이 늘어나는지 알고 싶다. pmap, grafana 를 통해 Heap, Non-Heap, DirectBuffer 를 확인해 봤다. $ pmap -x 1 | sort -k 3 -n -r | more pmap &amp;gt; JVM Heap &amp;gt; RSS 가 이전부터 Reserved Heap 수치에 도달한 상태이다. 증가분은 Heap 메모리가 아니다. grafana JVM Micrometer &amp;gt; Non-Heap 에 해당하는 메모리들 &amp;gt; 시계열상 사용량이 거의 일정하다. 증가분은 Non-Heap 메모리가 아니다. grafana JVM Micrometer &amp;gt; Direct Buffer &amp;gt; 시계열상 사용량이 거의 일정하다. 증가분은 Native Memory &amp;gt; DirectBuffer 도 아니다. 글을 정리하고 생각해보니 Native 메모리에 속하는 Direct Buffer 도 현상에 일부 영향을 줬을 수 있을 것 같다.즉, “JVM 에서 관찰할 수 없는 Native 메모리 어딘가”에서 메모리 사용량 증가가 일어나고 있다고 짐작할 수 있다.메모리 할당자 (malloc) 살펴보기앞선 포스트를 사내 게시판에도 게시했는데, 해당 포스트에 네이티브 메모리도 살펴보면 좋겠다는 동료 (팀장님) 분의 코멘트가 달린다.뒤이어 옆 파트에서도 유사 사례가 있었다는 내용을 전달 받는다.옆 파트에서 겪은 내용을 살펴보니 malloc (동적 메모리 할당자) 쪽에 메모리 파편화 이슈가 있다는 내용이다. JVM 을 사용하면서 나로선 전혀 생각지 못했을 영역의 이슈였다.사실 Native 메모리가 단순히 부족해서 생기는 문제가 아닐까 (사용량이 끝없이 증가하진 않을거니깐?) 라며 엉뚱한 곳에 초점을 두고 트러블 슈팅 중이었는데, 동료 (팀장님) 가 중간중간 방향을 잡아준 덕에 메모리 할당자 쪽에 영점을 맞춰 이부분을 파헤쳐 봤다. 포스팅에서 나도 이제 막 공부한 malloc 의 동작원리 따위를 다루어야 할지 말지 고민이 많았지만 문제를 이해하기 위해 필요한 내용이라고 판단했고 현상을 이해하기 위해 필요한 정도로만 다루어보기로 했습니다. 이에 대한 더 정확한 이해를 위해서는 glibc malloc 에 대한 공식 문서를 참고하시는 것을 권해드립니다.갑자기 메모리 할당자 (malloc) 요?OpenJDK 기준으로 JVM 은 OS 위에서 실행되는 C 프로세스이다. 때문에 C 메모리 할당 알고리즘의 영향을 받게 된다.다음 그림은 C / JVM Memory 매핑을 도식화한 그림이다. (출처)그림에서 보이듯 JVM 에서 Native / Non-heap 으로 표현되었던 메모리 영역들은 C 관점에서는 모두 Heap 에 위치한다.(JVM code, JVM Data, JVM Thread 의 Native Method Stack 등 일부 제외)JVM 은 C Heap 안에서 논리적으로 메모리를 구분하여 사용하는 것 뿐이다. 따라서 위에서 말한 “JVM 에서 관찰할 수 없는 Native 메모리 어딘가”에서 발생한 문제가 C 메모리 할당자 (malloc) 이슈와 관련이 있을 수도 있다는 점이 이상해보이진 않아졌다.메모리 할당자 종류대부분의 unix 기반 시스템은 표준 C 라이브러리 구현체로 glibc 를 사용하는데 glibc 는 동적 메모리 할당자로 ptmalloc2 을 사용한다. 그러니 대부분의 linux 배포판의 기본 메모리 할당자로 ptmalloc2 을 사용한다고 이해하면 된다.(alpine 등 일부 OS 는 제외 / 최상단 표 참고)그 밖에도 동적 메모리 (Heap) 할당자는 다양한 써드 파티에서 구현한 구현체들이 있다. 대표적인 몇가지만 보자면.. 메모리 할당자 Description 사용처 ptmalloc2 주로 glibc의 메모리 할당기로 사용되며, C/C++ 언어에서 사용되는 대규모의 동적 메모리 할당과 해제를 효율적으로 처리하는 것을 목표로 한다. 대부분의 Linux 배포판 (glibc) tcmalloc 다중 스레드 환경에서 뛰어난 성능을 발휘하는 것이 특징. TCMalloc은 각 스레드별로 메모리 캐시를 사용하여 메모리 할당과 해제를 더욱 빠르게 처리한다. Google jemalloc 다중 스레드 환경에서 효율적인 메모리 할당과 관리를 제공하며, 특히 대규모 멀티스레드 애플리케이션에서의 성능을 최적화한다. 낮은 메모리 오버헤드와 높은 확장성을 목표로 한다. Facebook, Redis, Firefox, … ptmalloc2 동작 원리뒤이어 설명할 이슈를 이해하기 위해서 ptmalloc2 의 동작 원리를 어느정도 알고 있는게 좋다. Terminology 설명 Arena 하나 혹은 그 이상의 쓰레드에서 공유하는 영역으로, 아레나에 할당된 쓰레드들은 아레나의 freelist 들로부터 메모리를 할당한다. Heap 인접해있는 영역의 메모리로 Chunk 들의 집합으로 이루어져 있다. 개별 힙 세그먼트는 정확히 하나의 아레나에 속하게 된다. Chunk 크게 In-Use 상태 (owned by app (JVM)) 혹은 Free 상태인 (owned by glibc) Chunk 로 구분할 수 있고 이들의 집합이 Heap 을 구성한다. freelist (= Bin) Free 상태인 Chunk 들을 관리하기 위한 연결 리스트 스레드가 메모리를 요청 malloc() 하면 하나의 Arena 를 할당받는데, 여러개의 스레드가 동일한 Arena 를 할당 받을 수 있다. 한번 할당받은 Arena 는 몇몇 예외 상황을 제외하면 스위치되지 않는다. ptmalloc2 은 이런 Arena 를 기본적으로 여러개 유지하기에 스레드들간 경합을 최소화할 수 있다.또한 하나의 Arena 에서 각 스레드는 별도의 Heap 세그먼트를 유지하기 때문에 이러한 Heap 을 유지하는 freelist (Bin) 도 별도로 유지된다. 이것이 멀티 스레딩 환경에서 동시에 malloc 요청이 오더라도 대개 즉각적으로 (= 경합 없이) freelist 로부터 메모리를 할당해 줄 수 있는 이유이다. dlmalloc 에서는 모든 스레드가 하나의 freelist 를 공유하기에 오직 하나의 스레드만이 임계 영역에 들어갈 수 있었다. 멀티스레딩 환경에서의 이런 경합을 개선하고자 나온것이 dlmalloc 을 포크한 ptmalloc2 이다.Arena 의 크기와 개수Arena 크기와 개수는 어떻게 정해질까?Arena 의 최대 크기 = 64MiB# 요즘은 대부분 64bit 시스템이므로 long == 8bytes 일 것이다.define DEFAULT_MMAP_THRESHOLD_MAX (4 * 1024 * 1024 * sizeof(long))define HEAP_MAX_SIZE (2 * DEFAULT_MMAP_THRESHOLD_MAX)# &quot;HEAP_MAX_SIZE&quot; = 2 * (4 * 1024 * 1024 * 8)# &quot;HEAP_MAX_SIZE&quot; = 67108864 &quot;bit&quot;# &quot;HEAP_MAX_SIZE&quot; = 65536 &quot;KiB&quot;# &quot;HEAP_MAX_SIZE&quot; = 64 &quot;MiB&quot;CPU 당 Arena 의 최대 개수 = 8define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))# long == 8bytes 이므로, CPU 당 ARENA 의 개수는 8 개다.즉, 64bit 시스템에서 CPU 하나당 Arena 의 최대 크기는 512MiB (64MiB * 8) 이다.우리 애플리케이션의 파드 cpu resource 는 10 이었다. 이는 Arena 의 개수가 최대 80개 까지 늘어날 수 있고 전체 크기는 최대 5GiB 이상 늘어날 수 있다는 이야기이다. 😱잡담여담이지만 cpu resource 리소스를 너무 많이 잡아 놓은 것 같다.JVM 애플리케이션 특성상 앱 시작시 많은 CPU 자원을 소모하기 때문에 cpu 리소스를 많이 할당했지만 평시에는 cpu 자원을 거의 사용하지 않았다.웜업 로직을 개선하든, 클러스터 버전업을 하면서 k8s 1.27 에 소개된 In-place Resource Resize 라는 피쳐를 사용해보든 개선해야 할 사항이라 생각한다.ptmalloc2 의 메모리 파편화 문제이제 위에서 언급한 malloc 의 메모리 파편화 이슈를 본격적으로 살펴본다.이슈를 두가지 면에서 살펴보고, 테스트를 통해 내용을 검증해보자.Point 1. Q. 위에서 계산한 수치는 Arena 의 “최대” 크기일 뿐 malloc() 후에 free() 만 잘 되고 있다면 문제될게 없지 않은가? A. 하지만 ptmalloc2 는 기대한 것처럼 동작하지 않는다. free() 가 호출되면 ptmalloc2 은 OS 로 (대체로) 메모리를 반환하지 않는다. 🤯대신 반환할 메모리를 향후 있을 수도 있을 사용을 위해 freelist 로 만드는데, freelist 는 OS 관점에서는 애플리케이션이 여전히 사용중인(RSS) 메모리일 뿐이다. 좀 더 복합적인 로직이 있지만 단순화하면, free() 호출 시, 향후 있을 사용을 위해 OS 로 메모리를 반환하지 않고 해당 Heap 을 freelist 로 할당한다. 이 말은 애플리케이션 메모리가 glibc 에 반환된 것이지, OS 에 반환된 것이 아니라는 말이다. 이같은 이유로 위에서 Chunk 를 설명할 때 Free Chunk 를 owned by glibc 로 표현했다. 👉 정리하면 free() 커맨드는 OS 로 메모리를 반환시키는 명령어라고 볼 수 없고, glibc 는 free() 된 메모리를 freelist 로 만드는데, 이는 여전히 애플리케이션에서 사용중인 (RSS) 메모리로 인식된다.Point 2. Q. OS 로 메모리가 반환되지 않더라도 freelist 재활용만 잘하면 문제될게 없지 않은가? A. 이 대목에서 glibc 의 메모리 점유와 관련된 버그 리포트들을 찾아볼 수 있다.메모리 할당자가 기대처럼 freelist 재활용을 잘 하지 못하고, 새로운 Arena 를 생성하면서 메모리 점유량이 늘어난다는 내용이다. https://github.com/cloudfoundry/java-buildpack/issues/320 https://github.com/quarkusio/quarkus/issues/36204 👉 요약하면 다음과 같은 일이 반복적으로 일어난다는 내용이다. 메모리 요청이 들어온다. 재사용을 위해 남겨둔 freelist 를 사용하지 않고 또 다른 Arena 를 생성한다. 메모리를 모두 사용한 뒤 (향후있게 될 사용을 위해) OS 에 반환하지 않고 freelist 로 만든다. Arena 에 사용되지 않는 freelist 들이 계속 쌓여간다.2번: freelist 가 실제로 재활용되지 않는 경우가 많다며 버그리포트에서 지적하는 내용이다.3번: glibc 문서에서 설명하고 있는 ptmalloc2 의 스펙이다.4번: 실제로 일어나고 있다면 메모리 (RSS)가 계속 증가하는 주범이 될 것이다. (검증 필요)malloc_trim()glibc 에 이런 freelist 를 실제 OS 로 반환하는 명령어로 malloc_trim() 명령어가 있다. 이는 외부에서 C-heap 을 trim 할 수 있도록 설계된 glibc 의 API 이다.JVM 에 위 같은 문제가 있자, malloc_trim() 명령어를 애플리케이션 레벨(JDK)에서 호출할 수 있게 해달라는 요청이 있었다. 이는 JDK 11.0.18, 17.0.2 에 백포팅돼서 jcmd 명령어를 통해 freelist 를 정리할 수 있게 되었다.👉 jcmd &amp;lt;pid&amp;gt; System.trim_native_heap https://bugs.openjdk.org/browse/JDK-8268893 https://bugs.openjdk.org/browse/JDK-8273602또한 JVM 에서 이를 자동으로 호출하는 기능이 JDK 17.0.12, 21.0.3 부터 정식 기능으로 백포팅되어 -XX:TrimNativeHeapInterval 옵션을 통해 trim 주기를 설정할 수 있다.초기에는 실험 기능으로 들어갔기 때문에 -XX:+UnlockExperimentalVMOptions 옵션을 함께 활성화 해야 했지만 지금은 공식 기능으로 채택되었다.기본으로는 비활성화되어있고, 주기를 설정하면 활성화된다. https://bugs.openjdk.org/browse/JDK-8293114 https://bugs.openjdk.org/browse/JDK-8325496Autotrim 기능은 JDK 11 엔 백포팅 해주지 않았다..😡 놓아줄 때가 된 듯 하다.테스트 &amp;amp; 검증테스트를 통해서 위에서 언급한 4번 (Arena 에 사용되지 않는 freelist 들이 계속 쌓여간다.) 을 검증해보자.메모리 할당 상태를 살펴보기 위해 pmap 커맨드를 이용해도 해당 메모리가 Arena 에 할당된 메모리인지 알기 힘든데, 다행히도 pmap 의 메모리 할당 상태를 분석해주는 java 애플리케이션이 있다. inspector 를 사용하면 100% 는 아니지만 대략적으로 현재 메모리의 할당 상태를 확인해볼 수 있다. pmap 결과를 인자로 전달하면 inspector 가 메모리 할당 상태를 분석해준다.분석 방법은 간단하다. 운영 파드(pod) 중 하나의 라벨을 변경하여 서비스에서 제외시킨다. jcmd &amp;lt;pid&amp;gt; System.trim_native_heap 전 후 pmap 을 비교한다. 애플리케이션 RSS 수치 변동 여부를 확인한다. trim 전 Memory mappings: JAVA_HEAP count=1 reserved=12058624 rss=11036196 UNKNOWN count=168 reserved=960548 rss=728168 MALLOC_ARENA count=259 reserved=16982044 rss=2221852 JAVA_THREAD count=226 reserved=232328 rss=20364 UNKNOWN_SEGMENT1 count=35 reserved=107660 rss=73952 UNKNOWN_SEGMENT2 count=63 reserved=129024 rss=118456 NON_JAVA_THREAD count=15 reserved=15480 rss=208 CODE_HEAP count=3 reserved=245760 rss=179680 trim 후 $ jattach 1 jcmd System.trim_native_heapConnected to remote JVMJVM response code = 0Attempting trim...Done.Virtual size before: 30741876k, after: 30741748k, (-128k)RSS before: 14379488k, after: 13552404k, (-827084k) &amp;lt;-- trimmed (단위: KB)Swap before: Ok, after: Ok, (Ok)Memory mappings: JAVA_HEAP count=1 reserved=12058624 rss=11035946 UNKNOWN count=168 reserved=960548 rss=728168 MALLOC_ARENA count=259 reserved=16982044 rss=1395884 &amp;lt;-- about 800 MB trimmed from ARENA (단위: KiB) JAVA_THREAD count=226 reserved=232328 rss=20284 UNKNOWN_SEGMENT1 count=35 reserved=107660 rss=73952 UNKNOWN_SEGMENT2 count=63 reserved=129024 rss=118456 NON_JAVA_THREAD count=15 reserved=15480 rss=208 CODE_HEAP count=3 reserved=245760 rss=179692 MALLOC_ARENA 개수가 259개, reserved 메모리가 16982044KiB 이다.16982044 KiB / 259 == 65567 KiB == 64.03 MiB 로 정확히 아레나의 최대 크기이다. 계산식 대로면 애플리케이션 할당 cpu 가 10개이기 때문에 최대 Arena 는 80개여야 하는데..?개수가 훨씬 많아서 이상하다가도, reserved 크기를 보면 아레나가 맞다. 이부분은 왜인지 아직 모르겠지만, 개수보다는 RSS 수치가 중요하기 때문에 우선 넘어간다. 애플리케이션 RSS 수치 확인수일간 점진적으로 증가해오던 메모리 수치가 trim 후, 훅 떨어졌다. 앞서 설명했듯이 JDK 17, 21 을 사용하면 이것을 일정주기로 auto trim 해줄 수 있다. JDK 11 에선 jcmd 명령어를 주기적으로 실행해주는 스크립트를 작성해도 문제가 없을 것 같지만 이 참에 JDK 버전을 올리는 방향이 더 좋아보인다.이로써 실제 trim 을 통해 MALLOC_ARENA 에서 대략 800MB 가량의 메모리가 OS 로 반환되었음을 확인했다. 이말은 JVM pid == 1 에 820MB 가량의 메모리가 freelist (사용되지 않는 상태) 상태로 남아있었다는 말이다.또한 애플리케이션의 에이징됨에 따라 정리되는 (trimmed) 메모리가 커지는 것을 관찰할 수 있었는데 이를 통해 freelist 가 시간이 지남에 따라 축적된다는 점을 알 수 있었고, 위에서 말한 4번 (Arena 에 사용되지 않는 Freelist 들이 계속 쌓여간다.)을 경험적으로 검증한 셈이다.정리 현상: JVM 수준에서 관측할 수 없는 Native 메모리 사용량의 지속적인 증가 현상 원인: glibc 를 표준 C 라이브러리로 사용하는 리눅스 시스템 C 메모리 할당자 (= ptmalloc2) 의 메모리 파편화 문제개선 방안👉 JDK 17.0.12, 21.0.3 이상 버전 -XX:TrimNativeHeapInterval 옵션을 활성화 하여 JVM 이 파편화된 freelist 메모리를 주기적으로 정리하도록 한다.👉 그 이전 버전 JDK 17.0.12, 21.0.3 이상 버전으로 업그레이드한다. 😄 jcmd 의 System.trim_native_heap 를 호출하는 스크립트를 일정 주기로 실행시킨다.JDK 11.0.18, 17.0.2 이상의 jcmd 에만 포함된 기능이다. 시스템의 메모리 할당자를 변경한다. (jemalloc, tcmalloc)이런(사용해본적 없는 메모리 할당자를 사용하는) 부담을 가지고 갈바엔 JDK 업데이트를 하는게 나을 것 같다. 시스템 최대 ARENA 개수(MALLOC_ARENA_MAX)를 조정한다.C Heap 을 제한하는 방식이라 퍼포먼스 이슈가 있을 수도 있다.이 방식을 택하려면 상황을 잘 고려해서 사용해야 할 것 같다. Note. 이론상 CPU 가 하나 추가될 때마다 Native 메모리가 512MiB 까지 더 필요할 수도 있다는 점사실 ptmalloc2 이 항상 freelist 를 재활용하지 않는 것도 아니고, OS 로 메모리 반환을 항상 하지 않는 것도 아니다. 때문에 메모리 점유율은 증가하긴 하지만 완만하고 느리다.배포 주기가 매우 긴 조직이 아니면 모든 Arena 가 이론상의 최대 수치 만큼 도달하지는 않을 것 같다. 중요한 점은 문제를 인지하고 이에 대한 대비를 하느냐의 여부일 것 같다. ptmalloc2 는 (JVM 을 구동할 때?) 메모리 파편화 이슈가 있다는 점사실은 ptmalloc2 자체의 “이슈”라기 보다는 ptmalloc2 의 메모리 할당 알고리즘이 JVM 과 핏하지 않은 것 뿐이라는 생각이 더 크다. JVM 17.0.9, 21.0.1 이상 버전에서는 -XX:TrimNativeHeapInterval 을 세팅하는게 좋다는 점위 세팅은 기본적으로 disable 되어있다. Native 메모리 파편화를 방지하기 위해서 세팅해두면 좋을 것 같다. " }, { "title": "JVM 메모리 누수 &gt; Heap &gt; lettuce", "url": "/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Heap-lettuce/", "categories": "JVM", "tags": "JDK, JVM, leak, MAT, jcmd", "date": "2024-04-12 21:00:00 +0900", "snippet": "최근 회사 애플리케이션의 JVM 메모리 누수를 찾아낸 뒤 픽스했다.과정을 정리해두면 메모리 누수를 또 찾고 있을 미래의 나에게도 도움이 될 것 같아 정리해본다.트러블 슈팅 배경애플리케이션 파드가 일정 시간 이상 에이징이 되면 RESTART 되고 있다.사내 메트릭 분석 툴에 힙 히스토그램을 시계열로 분석할 수 있는 기능이 있어 이를 활용해봤다.임의의 파드를 여러개 분석해 본 결과 long[] 타입 객체가 시간이 지나면서 힙에 계속 누적되는 현상을 알 수 있었는데, 문제는 이 객체들이 어디서부터 생성되는지다.원인 파악 과정우선 힙 덤프를 수행해야 한다. 운영 환경에서만 발생하는 문제이기 때문에 운영 환경 컨테이너에 힙 덤프를 수행해야 하는데, 힙 덤프를 수행하면 메모리 스냅샷을 기록하기 위해 모든 스레드가 중단되는 stop-the-world 가 발생한다.운영 환경에 영향을 주지 않고 이를 수행하기 위해서 분석하고자 하는 파드의 라벨을 변경하여 서비스에서 제외시킨다음 힙 덤프를 수행했다.VisualVM &amp;gt; 힙 덤프초기에는 VisualVM 으로 힙 덤프를 수행했으나, 덤프 크기를 최소화하기 위해서인지 덤프 수행 전 항상 Full GC 를 수행했다. 그 결과 분석하고자 하는 long[] 객체들도 메모리에서 모조리 해제되고 100MB 이하만 남게 되었다.그래도 혹시 모르니 그나마 남아있는 long[] 객체들의 레퍼런스를 확인해봤다.long[] 의 레퍼런스 상당수가 lettuce 패키지와 관련되어 있다.jcmd &amp;gt; 힙 덤프jcmd 로 덤프 실행 시점 그대로의 메모리 스냅샷을 얻어낼 수 있는 방법이 있어서 이 방법대로 힙 덤프를 수행했다. 전체 덤프룰 수행하니 덤프 파일 용량이 9G 에 육박했다.MAT &amp;gt; 덤프파일 분석 long[] 객체가 의미 있는 단위로 증가할 때마다 힙 덤프를 수행(120MB, 450MB, 1.3GB) 덤프 파일을 파드에서 로컬로 복사하여 MAT 를 활용하여 덤프 파일 분석 long[] 타입 객체의 Immediate Dominator 를 파악하여 어디에 기인하는 객체인지 파악long[] &amp;lt;= 120MB 인 시점long[] &amp;lt;= 450MB 인 시점long[] &amp;lt;= 1.3GB 인 시점세 덤프 파일 모두 long[] 타입 객체들의 Immediate Dominator 를 따라가보니 Lettuce CommandLatencyRecorder 에서 사용하는 컴포넌트들 (Hstogram, AtomicHistogram, LatencyUtils) 의 파이가 앱이 에이징됨에 따라 점점 누적되고 있었다. lettuce 가 용의자에서 범인이 되는 순간이다. 😊 유사 사례해결 방법lettuce 의 Command Latency Metrics 스펙은 우리 파트에서 직접 사용하지 않고 앞으로 사용하지 않을 것 같은 lettuce 의 디버깅, 모니터링 관련 스펙이라고 판단했고 이에 따라 기능을 disable 처리했다....val clientResources = DefaultClientResources.create().mutate() .commandLatencyRecorder(CommandLatencyRecorder.disabled()) // recorder 기능을 disabled 시킨다. .build()return LettucePoolingClientConfiguration.builder() .clientResources(clientResources) // 위에서 mutate 한 client resources 를 빌더에 추가한다. .build()...패치 이후 메트릭 모니터링앱이 에이징 되어도 이전처럼 long[] 타입 객체가 누적되지 않음을 확인했다.부록전체 덤프 파일을 불러와도 MAT 에서 힙 분석 결과를 간소화 시키는 경우MAT 에서 Unreachable Objects 까지 모두 트래킹하도록 옵션을 변경해줘야 한다. Preferences (Setting) &amp;gt; Memory Analyzer &amp;gt; Keep unreachable objects이렇게 하면 나처럼 9GB 짜리 덤프 파일을 불러오다가 힙 부족으로 계속 실패를 할 수도 있는데 😅 그땐 MAT 애플리케이션에서 패키지 내용 보기 &amp;gt; Contents &amp;gt; Eclipse &amp;gt; MemoryAnalyzer.ini 파일을 열고 xmx 을 늘려주면 된다." }, { "title": "JVM Container Awareness Bug (cgroup v2) &gt; JDK 11.0.16", "url": "/posts/JVM-Container-Awareness-Bug-cgroup-v2-JDK-11.0.16/", "categories": "JVM", "tags": "JDK, JDK11, JVM, Kubernetes, docker, Container Awareness, cgroup", "date": "2022-11-05 16:54:00 +0900", "snippet": "저처럼 글 읽기를 싫어하시는 분들을 위한 결론 Kubernetes 환경에서 JDK 11 을 사용중이라면 JDK 11.0.16 이상의 버전 사용을 권장한다. JDK 12 - 14 버전을 사용중이라면 15 이상의 버전 사용을 권장한다. 그렇지 않으면 소중한 Pod 가 OOMKilled 당할 수 있다. 특히 InitialRAMPercentage, MaxRAMPercentage 옵션을 사용한다면 꼭! 글이 깁니다. 다음 내용에 대해 구구절절 설명합니다. k8s 환경에서 JDK 11 구동시, 왜 JDK 11.0.16 이상의 버전을 사용해야 하는지 이 내용을 알아가게 된 과정 “적절한” 리소스 산정을 위한 여정…최근 회사에서 메인 프로젝트 배포를 앞두고 나름대로 배포 프로세스를 정비 중이었다. 배포할 애플리케이션은 서비스에서 발생하는 대부분의 트래픽을 처리하는 메인 애플리케이션이었고, 이 거대한 시스템을 새로운 언어로 포팅하여 마침내 배포를 앞둔 단계였다. 때문에 Kubernetes Pod 의 “적절한” 리소스 산정 작업이 꼭 필요했다.“적절한” 리소스 산정은 어떻게 해야 할까?Pod 야.. OOMKilled 로부터 지켜주지 못해 미안해 Pod 가 왜 설정한 메모리 limit (8Gi) 보다 더 많은 메모리를 사용하다 죽는걸까? 분명 MaxRAMPercentage == 70(%) 으로 지정했는데 😥 과거 JVM 기반의 애플리케이션 리소스 산정을 위해 JMX 로 애플리케이션 모니터링도 해보고 Heap 덤프도 했었다. 나름의 삽질을 통해 리소스를 산정하여 k8s 위에 애플리케이션을 올렸지만 종종 알 수 없는 이유로 내 소중한 Pod 가 OOMKilled 당하는 모습을 목격해야 했고, 결국엔 리소스 상향이라는 미봉책으로 여차 저차 땜질했던 기억이 다시 떠올랐다.InitialRAMPercentage, MaxRAMPercentage 그리고 JDK 11지난번 부족했던 점을 복기하고자, 당시 사내 게시판에 작성한 삽질기에 전 파트 동료분이 작성해주신 댓글과 링크를 열심히 곱씹어 봤다. 당시 애플리케이션의 Heap 메모리 조절을 위한 JVM 옵션 값으로 InitialRAMPercentage, MaxRAMPercentage 를 사용하고 있었는데, xmx, xms 옵션보다 유동적이라는 이유에서였다. 힙을 비율로 지정하면 k8s 메모리 리소스를 조정할 때 따로 힙 크기를 손대지 않아도 되기 때문이다.JDK 13 부터 해당 옵션을 사용할 수 있다? (X)궁금해서 코멘트에 참조된 버그 리포트를 쭉 읽어봤다. 시스템(컨테이너) 메모리가 JVM MaxRAM 보다 큰 경우 위 옵션은 시스템 메모리가 아닌 MaxRAM 을 기준으로 MazHeapSize 가 결정된다. (MaxHeapSize = MaxRAM * (MaxRAMPercentage) / 100) 1번 문제가 해결되더라도 MaxRAMPercentage 로 설정된 MaxHeapSize 가 32g 를 넘어갈 수 없도록 되어있다. 이는 기본적으로 활성화돼있는 UseCompressedOops 옵션이 해당 옵션들 보다 우선되기 때문이다. 내용을 정리하면, JDK 13 이하 버전에서 InitialRAMPercentage, MaxRAMPercentage 옵션을 사용할 수 없다는 내용이 아니라, 일부 조건 하에서 Heap 비율 산정이 제대로 이루어지지 않는 다는 내용의 버그리포트이다.JDK 11 에서 사용 가능하다.특정 조건이란 다음 두가지인데 우리 애플리케이션은 이에 해당되지 않음을 확인했다. 시스템 메모리가 MaxRAM 보다 큰 경우 Heap 사이즈가 32g 를 넘어가는 경우 확인해 보니 MaxRAM 의 default 값은 128Gi 이다. 우리는 MaxRAM 보다 작은 10Gi 내외의 시스템 메모리 (= 파드 리소스 기준) 를 사용한다. 시스템 메모리가 더 작을 땐 MaxRAM 이 아닌 시스템 메모리를 기준으로 MaxHeapSize 를 산출한다. 앞서 말했듯 시스템 메모리는 10Gi 내외로 산정할 것이기 때문에 힙사이즈가 32g 이상 될 일이 없다. 이를 통해 InitialRAMPercentage, MaxRAMPercentage 옵션 사용에 문제가 없음을 확인했다.이상한 MaxHeapSize위 흐름을 이어 받아, 이전에 OOMKilled 로 고생하던 애플리케이션의 openjdk 버전에 MaxRAMPercentage 옵션을 넣고 MaxHeapSize 를 출력해봤다. 컨테이너 메모리: 1GB MaxRAMPercentage: 80docker-desktop 4.3.0openjdk11.0.4_11MaxHeapSize == 6.13G 라는 엉뚱한 수치가 나왔다. (..???)openjdk13.0.2_8심지어 JDK 13 도 마찬가지다.JVM Container Awareness 버그이 이상한 현상에 대해 또 다시 구글링을 해봤다.아티클 하나를 보고 이 또한 버그임을 알게 됐다. 버그 내용을 요약하면, 다음과 같다. jvm 의 컨테이너 리소스 detection 은 cgroup v1 위에 설계됐지만 cgroup v1 을 사용하던 Docker 가 새 버전을 릴리즈하면서 일부 cgroup v2 feature 를 사용하게 되었다.따라서 그 위에서 cgroup v2 호환성 대응이 되어있지 않은 버전의 jvm 을 구동하면 컨테이너 리소스 detection 에 문제가 생길 수 있다. cgroup은 단일 또는 태스크 단위의 프로세스 그룹에 대한 자원 할당을 제어하는 리눅스 커널 모듈이고 v1, v2 가 있다.ref. https://sonseungha.tistory.com/535관련 내용이 도커 문서에도 나와있다. docker-desktop 4.3.0 부터 cgroup v2 feature 를 사용한 것으로 보인다.위 openJDK 버그 리포트에 따르면 이 버그는 JDK 15 에서 패치됐지만 아주 다행히도 최근 11.0.16 GA 버전에 백포팅됐다고 한다. 아무래도 컨테이너 환경에서 구동되는 애플리케이션들이 많은 요즘에는 크리티컬한 버그이고 11 은 LTS 여서 백포팅해준 듯 하다.위 도커 테스트에서는 docker-desktop 버전이 4.11.0 으로 cgroup v2 feature 를 사용중일 것이고, JVM 는 그에 대한 대응이 되지 않은 11.0.4, 13.0.2_8 버전이어서 컨테이너 리소스 detection 이 제대로 되지 않았던 것이다.그럼 JDK 15 와 11.0.16 버전은 힙 사이즈 산출을 제대로 할까?openjdk15.0.2_7openjdk11.0.16.1_1예상대로 MaxHeapSize 를 잘 산출해낸다.docker-desktop 3.6.0그럼 반대로 docker-desktop 버전을 내린다면 JDK 11.0.4, 13.0.2_8 에서도 잘 동작해야 할 것이다.openjdk11.0.4_11openjdk13.0.2_8앞서 말했듯이 docker-desktop 3.6.0 은 cgroup v2 feature 를 사용하지 않는다. 따라서 cgroup v1 위에 설계된 JDK 11.0.4, 13.0.2_8 의 메모리 detection 이 아까와는 다르게 잘 동작한다.Kubernetes 와 JVM 애플리케이션의 OOMKilled그런데 아까 봤던 아티클에서 눈에 띄는 대목이 있다. Kubernetes 도 cgroup v2 feature 를 사용하는데, 위 Container Awareness 버그가 픽스되지 않은 버전의 JVM 을 구동하게 되면 (Pod 가 아닌) Node 기준으로 MaxHeapSize 를 산출할 수 있고 그렇게 되면 JVM 애플리케이션이 Pod resource limit 을 넘게되어 OOMKilled 당할 수도 있다. 도커 테스트에서 컨테이너 limit 인 1GB 와 관계없이 MaxHeapSize 가 쌩뚱 맞게 6.13G 로 잡힌 것을 기억할 것이다.앞서 과거에 배포한 애플리케이션이 알 수 없는 이유로 OOMKilled 됐다고 했는데, 문제의 원인이 cgroup v2 대응이 되지 않은 openjdk 버전(11.0.4)을 사용한 것일 수도 있다는 생각이 들어 두근두근(?)하기도 했다.그렇다면 Kubernetes 는 언제부터 cgroup v2 를 사용했고 무얼할 때 cgroup v2 를 사용하는지 궁금하여 쿠버네티스 문서를 찾아봤다.cgroup v2 를 사용한 건 v1.25 [stable] 버전부터인 것 같고 Memory QoS 에 cgroup v2 feature 를 사용한다고 한다. 쿠버네티스 문서 문서 마지막에 대놓고 JDK 15 이후 버전 혹은 11.0.16 버전을 사용하라고 명시돼있다. (처음부터 이 문서를 봤었으면…😇) 그리고 MemoryQoS 문서에는 위와 같은 내용이 나와있다.어찌됐든 이제 새로 배포할 프로젝트 및 기존 JDK 11 애플리케이션의 마이너 버전을 변경 (11.x.x -&amp;gt; 11.0.16) 해야 할 이유를 찾았다.정리Container Awareness 가 제대로 이루어짐으로써 아래 두가지를 기대해 볼 수 있다. 이번에 배포할 애플리케이션 JDK 버전을 11.0.16 으로 업그레이드 하고 위 JVM 옵션을 사용하면 “적절한” 리소스 산출시 보다 더 정확한 도움을 받을 수 있을 것 같다. (이전 JDK 버전의 resource detection 이 부정확했기 때문에) 더불어 Kubernetes 클러스터에서 알 수 없는 이유로 발생하던 OOMKilled 이슈도 단순 리소스를 상향 조정하는 미봉책 없이 자연스럽게 해결될 수 있다고 살짝 기대해본다. " }, { "title": "spring.web.resources.add-mappings 옵션", "url": "/posts/spring.web.resources.add-mappings-%EC%98%B5%EC%85%98/", "categories": "Spring, DispatcherServlet", "tags": "SpringBoot, DispatcherServlet, HandlerMapping", "date": "2022-11-05 05:52:00 +0900", "snippet": "기대와 다른 DispatcherServlet 의 동작 방식회사에서 특정 엔드포인트에 접속 시 톰캣 에러페이지가 그대로 노출되는 이슈를 디버깅하다가 DispatcherServlet 의 기이한(?) 동작을 발견하고 왜 그런지 궁금하여 알아봤다.컨트롤러가 바인딩되지 않은 경로로 접속을 하면 당연히 DispatcherServlet 에서 NoHandlerFoundException 이 발생할 것으로 기대했지만 실제 동작은 그렇지 않았다. 오히려 HandlerMapping 단계에서 ResourceHttpRequestHandler 라는 핸들러를 아주 잘 매핑하고 있었다.구글링 후 아래 설정에 따라 동작이 달라진다는 사실을 알 수 있었고, 기본값은 true 다.spring.web.resources.add-mappings: true (default)핸들러가 없으면 Resource 경로로 매핑한다.위 설정 하에서는 매핑되는 컨트롤러(=핸들러)가 없더라도 SimpleUrlHandlerMapping.getHandler 에 의해 ResourceHttpRequestHandler 가 매핑된다. 즉, 핸들러가 없는 /no-path-like-this 라는 경로에 요청이 들어왔을 때, 이 경로를 Resource 경로로 판단한다는 뜻이다.물론 Resource 디렉토리에 이에 해당하는 리소스가 없을 것이니 404를 잘 응답하겠지만, 존재하지 않는 모든 경로를 Resource 경로로 인식하게 끔 하는 것은 내가 의도한 바가 아니었다.이 같은 동작을 피하기 위해서는 spring.web.resources.add-mappings: false 로 설정하면 된다.Why?SimpleUrlHandlerMapping 이 생성될 때 urlMap 을 생성자로 전달 받는대, 이 값은 스프링 애플리케이션이 실행될 때 SimpleUrlHandlerMapping.registerHandlers 에 의해 SimpleUrlHandlerMapping.handlerMap 에 등록된다.urlMap (이해를 위해 JSON 으로 표현){ &quot;/webjars/**&quot;: &quot;ResourceHttpRequestHandler&quot;, &quot;/**&quot;: &quot;ResourceHttpRequestHandler&quot;, &quot;/static/**&quot;: &quot;ResourceHttpRequestHandler&quot;}따라서 런타임에 실제로 없는 경로 (e.g. /no-path-like-me)로 요청이 들어오면 해당 경로가 /** 에 매치되면서 ResourceHttpRequestHandler 를 반환하게 되는것이다." } ]
