<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc" /><meta property="og:locale" content="en" /><meta name="description" content="몇 개월 전에 회사 애플리케이션에서 발생한 메모리 누수를 트러블 슈팅 &amp; 픽스한 내용을 포스트로 다루었다. 누수를 잡아낸 후에도 미량이지만 애플리케이션 전체 메모리 사용량 (RSS 기준)이 계속해서 늘어나고 있었다." /><meta property="og:description" content="몇 개월 전에 회사 애플리케이션에서 발생한 메모리 누수를 트러블 슈팅 &amp; 픽스한 내용을 포스트로 다루었다. 누수를 잡아낸 후에도 미량이지만 애플리케이션 전체 메모리 사용량 (RSS 기준)이 계속해서 늘어나고 있었다." /><link rel="canonical" href="https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/" /><meta property="og:url" content="https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/" /><meta property="og:site_name" content="작심삼일 블로그" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-07-18T20:54:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc" /><meta name="twitter:site" content="@" /><meta name="google-site-verification" content="sGZlAxCFoyiY1YBBAqfGj5urtMWcA4HzjhZvcJAajOU" /> <script type="application/ld+json"> {"dateModified":"2024-10-30T20:48:26+09:00","datePublished":"2024-07-18T20:54:00+09:00","description":"몇 개월 전에 회사 애플리케이션에서 발생한 메모리 누수를 트러블 슈팅 &amp; 픽스한 내용을 포스트로 다루었다. 누수를 잡아낸 후에도 미량이지만 애플리케이션 전체 메모리 사용량 (RSS 기준)이 계속해서 늘어나고 있었다.","mainEntityOfPage":{"@type":"WebPage","@id":"https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/"},"url":"https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/","@type":"BlogPosting","headline":"JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc","@context":"https://schema.org"}</script><title>JVM 메모리 누수 > Native 메모리 > glibc malloc | 작심삼일 블로그</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="작심삼일 블로그"><meta name="application-name" content="작심삼일 블로그"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/profile.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">작심삼일 블로그</a></div><div class="site-subtitle font-italic">진짜 작심삼일이 될 줄이야..</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/leeby-devel" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['leeby.devel','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/byunghun-lee/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>JVM 메모리 누수 > Native 메모리 > glibc malloc</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>JVM 메모리 누수 > Native 메모리 > glibc malloc</h1><div class="post-meta text-muted"><div> By <em> <a href="https://twitter.com/username">leeby</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1721303640" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2024-07-18 </em> </span> <span> Updated <em class="timeago" data-ts="1730288906" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2024-10-30 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5066 words"> <em>28 min</em> read</span></div></div></div><div class="post-content"><p>몇 개월 전에 회사 애플리케이션에서 발생한 메모리 누수를 트러블 슈팅 &amp; 픽스한 내용을 <a href="/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Heap-lettuce/" target="_blank">포스트</a>로 다루었다. 누수를 잡아낸 후에도 미량이지만 애플리케이션 전체 메모리 사용량 (RSS 기준)이 계속해서 늘어나고 있었다.</p><p>원인을 파헤쳐보니 크리티컬한 내용은 아니나, GNU C 라이브러리 (glibc) 위에 구동되는 JVM 은 알게 모르게 영향을 받을 내용이라서 이곳에 그 내용을 공유해본다.</p><blockquote class="prompt-note"><div><p><em>글의 내용이 glibc 를 사용하는 시스템을 전제로 하기 때문에 이 부분을 먼저 확인하시면 좋을 것 같습니다.</em></p></div></blockquote><p><img width="864" alt="스크린샷 2024-07-25 오후 8 39 45" data-src="https://github.com/user-attachments/assets/d4917d5a-4503-4242-b9b2-8779ded9f93d" data-proofer-ignore> <em>대표적으로 많이 사용하는 OS, JDK 벤더 기준으로 표를 구성해봤다.</em></p><details> <summary>잡담</summary> adoptopenjdk/alpine 의 경우 alpine 의 기본 C 라이브러리인 musl 의 안정성 검토가 다 되지 않아 <a href="https://github.com/AdoptOpenJDK/openjdk-docker/blob/master/15/jdk/alpine/Dockerfile.hotspot.releases.full">JDK 15 이전까지는 glibc 를 별도로 설치</a>해서 사용해왔다. JDK 16 부터는 musl 을 사용한다고 하는데, 호환성이 좋을지는 모르겠다. 향후 파트 내 시스템을 JDK 21 로 마이그레이션할 계획인데 이땐 alpine 대신 debian-slim 을 사용해야겠다. </details><h2 id="끝나지-않은-메모리-증가-현상">끝나지 않은 메모리 증가 현상 <a href="#끝나지-않은-메모리-증가-현상" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>메모리 누수를 픽스한 이후 수 일간의 파드 모니터링 과정에서 파드가 죽고있거나 메모리 사용량이 눈에 띄게 증가하진 않았다. 하지만 왜인지 모를 이유로 메모리 사용량이 미량이지만 아주 꾸준하게 늘어나고 있다.</p><p><img width="839" alt="스크린샷 2024-07-18 오후 7 24 17" data-src="https://github.com/user-attachments/assets/a4b882a7-a492-4476-914a-e68ef0f01063" data-proofer-ignore> <em>도대체 뭘까?</em></p><h2 id="heap-non-heap-direct-buffer-확인">Heap, Non-Heap, Direct Buffer 확인 <a href="#heap-non-heap-direct-buffer-확인" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>우선 메모리의 어떤 영역이 늘어나는지 알고 싶다. pmap, grafana 를 통해 Heap, Non-Heap, DirectBuffer 를 확인해 봤다.</p><blockquote><p><code class="language-plaintext highlighter-rouge">$ pmap -x 1 | sort -k 3 -n -r | more</code></p><p><img width="839" alt="스크린샷 2024-07-23 오후 3 51 47" data-src="https://github.com/user-attachments/assets/003af26c-eaf9-4444-bf66-54c72460dc54" data-proofer-ignore></p><p>pmap &gt; JVM Heap &gt; RSS 가 이전부터 Reserved Heap 수치에 도달한 상태이다.</p><ul><li>증가분은 <strong>Heap 메모리가 아니다</strong>.</ul><p>grafana JVM Micrometer &gt; Non-Heap 에 해당하는 메모리들 &gt; 시계열상 사용량이 거의 일정하다.</p><ul><li>증가분은 <strong>Non-Heap 메모리가 아니다</strong>.</ul><p>grafana JVM Micrometer &gt; Direct Buffer &gt; 시계열상 사용량이 거의 일정하다.</p><ul><li>증가분은 Native Memory &gt; <strong>DirectBuffer 도 아니다</strong>.</ul><p><img width="839" alt="스크린샷 2024-07-18 오후 8 24 43" data-src="https://github.com/user-attachments/assets/2fd1cbe6-9d56-4646-9db1-3ae29a412aa9" data-proofer-ignore> <em>글을 정리하고 생각해보니 Native 메모리에 속하는 Direct Buffer 도 현상에 일부 영향을 줬을 수 있을 것 같다.</em></p></blockquote><p>즉, <strong>“JVM 에서 관찰할 수 없는 Native 메모리 어딘가”</strong>에서 메모리 사용량 증가가 일어나고 있다고 짐작할 수 있다.</p><h1 id="메모리-할당자-malloc-살펴보기">메모리 할당자 (malloc) 살펴보기</h1><p><img data-src="https://github.com/user-attachments/assets/1da0919b-dd4f-481d-a965-b64f891ba00c" alt="KakaoTalk_Photo_2024-07-25-21-49-27" data-proofer-ignore> <em>앞선 포스트를 사내 게시판에도 게시했는데, 해당 포스트에 네이티브 메모리도 살펴보면 좋겠다는 동료 <del>(팀장님)</del> 분의 코멘트가 달린다.</em></p><p><img data-src="https://github.com/user-attachments/assets/1138f550-d28a-46c8-9b1a-b75768876e10" alt="KakaoTalk_Photo_2024-07-19-20-48-05" data-proofer-ignore> <em>뒤이어 옆 파트에서도 유사 사례가 있었다는 내용을 전달 받는다.</em></p><p>옆 파트에서 겪은 내용을 살펴보니 malloc (동적 메모리 할당자) 쪽에 <strong>메모리 파편화 이슈</strong>가 있다는 내용이다. JVM 을 사용하면서 나로선 전혀 생각지 못했을 영역의 이슈였다.</p><p>사실 Native 메모리가 단순히 부족해서 생기는 문제가 아닐까 (사용량이 끝없이 증가하진 않을거니깐?) 라며 엉뚱한 곳에 초점을 두고 트러블 슈팅 중이었는데, 동료 <del>(팀장님)</del> 가 중간중간 방향을 잡아준 덕에 <strong>메모리 할당자 쪽에 영점을 맞춰</strong> 이부분을 파헤쳐 봤다.</p><blockquote class="prompt-note"><div><p>포스팅에서 나도 이제 막 공부한 malloc 의 동작원리 따위를 다루어야 할지 말지 고민이 많았지만 문제를 이해하기 위해 필요한 내용이라고 판단했고 현상을 이해하기 위해 필요한 정도로만 다루어보기로 했습니다. 이에 대한 더 정확한 이해를 위해서는 <strong><a href="https://sourceware.org/glibc/wiki/MallocInternals" target="_blank">glibc malloc 에 대한 공식 문서</a></strong>를 참고하시는 것을 권해드립니다.</p></div></blockquote><h2 id="갑자기-메모리-할당자-malloc-요">갑자기 메모리 할당자 (malloc) 요? <a href="#갑자기-메모리-할당자-malloc-요" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>OpenJDK 기준으로 JVM 은 OS 위에서 실행되는 C 프로세스이다. 때문에 C 메모리 할당 알고리즘의 영향을 받게 된다.</p><p><img width="872" alt="스크린샷 2024-07-19 오후 10 12 21" data-src="https://github.com/user-attachments/assets/459c16df-a2d7-4b8a-ac7c-6ddf34ff6683" data-proofer-ignore> <em>다음 그림은 C / JVM Memory 매핑을 도식화한 그림이다. (<a href="https://www.alibabacloud.com/blog/explaining-memory-issues-in-java-cloud-native-practices_599957" target="_blank">출처</a>)</em></p><p>그림에서 보이듯 JVM 에서 Native / Non-heap 으로 표현되었던 메모리 영역들은 C 관점에서는 모두 Heap 에 위치한다.<br /> (JVM code, JVM Data, JVM Thread 의 Native Method Stack 등 일부 제외)</p><p>JVM 은 C Heap 안에서 논리적으로 메모리를 구분하여 사용하는 것 뿐이다. 따라서 위에서 말한 <strong>“JVM 에서 관찰할 수 없는 Native 메모리 어딘가”</strong>에서 발생한 문제가 C 메모리 할당자 (malloc) 이슈와 관련이 있을 수도 있다는 점이 이상해보이진 않아졌다.</p><h2 id="메모리-할당자-종류">메모리 할당자 종류 <a href="#메모리-할당자-종류" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>대부분의 unix 기반 시스템은 표준 C 라이브러리 구현체로 glibc 를 사용하는데 glibc 는 동적 메모리 할당자로 ptmalloc2 을 사용한다. 그러니 <strong>대부분의 linux 배포판의 기본 메모리 할당자로 ptmalloc2 을 사용한다</strong>고 이해하면 된다.<br /> <em>(alpine 등 일부 OS 는 제외 / 최상단 표 참고)</em></p><p>그 밖에도 동적 메모리 (Heap) 할당자는 다양한 써드 파티에서 구현한 구현체들이 있다. 대표적인 몇가지만 보자면..</p><div class="table-wrapper"><table><thead><tr><th>메모리 할당자<th>Description<th>사용처<tbody><tr><td><strong>ptmalloc2</strong><td>주로 glibc의 메모리 할당기로 사용되며, C/C++ 언어에서 사용되는 대규모의 동적 메모리 할당과 해제를 효율적으로 처리하는 것을 목표로 한다.<td>대부분의 Linux 배포판 (glibc)<tr><td>tcmalloc<td>다중 스레드 환경에서 뛰어난 성능을 발휘하는 것이 특징. TCMalloc은 각 스레드별로 메모리 캐시를 사용하여 메모리 할당과 해제를 더욱 빠르게 처리한다.<td>Google<tr><td>jemalloc<td>다중 스레드 환경에서 효율적인 메모리 할당과 관리를 제공하며, 특히 대규모 멀티스레드 애플리케이션에서의 성능을 최적화한다. 낮은 메모리 오버헤드와 높은 확장성을 목표로 한다.<td>Facebook, Redis, Firefox, …</table></div><h2 id="ptmalloc2-동작-원리">ptmalloc2 동작 원리 <a href="#ptmalloc2-동작-원리" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>뒤이어 설명할 이슈를 이해하기 위해서 ptmalloc2 의 동작 원리를 어느정도 알고 있는게 좋다.</p><div class="table-wrapper"><table><thead><tr><th>Terminology<th>설명<tbody><tr><td>Arena<td>하나 혹은 그 이상의 쓰레드에서 공유하는 영역으로, 아레나에 할당된 쓰레드들은 아레나의 freelist 들로부터 메모리를 할당한다.<tr><td>Heap<td>인접해있는 영역의 메모리로 Chunk 들의 집합으로 이루어져 있다. 개별 힙 세그먼트는 정확히 하나의 아레나에 속하게 된다.<tr><td>Chunk<td>크게 In-Use 상태 (<strong>owned by app (JVM)</strong>) 혹은 Free 상태인 (<strong>owned by glibc</strong>) Chunk 로 구분할 수 있고 이들의 집합이 Heap 을 구성한다.<tr><td>freelist (= Bin)<td>Free 상태인 Chunk 들을 관리하기 위한 연결 리스트</table></div><p>스레드가 메모리를 요청 <code class="language-plaintext highlighter-rouge">malloc()</code> 하면 하나의 Arena 를 할당받는데, 여러개의 스레드가 동일한 Arena 를 할당 받을 수 있다. 한번 할당받은 Arena 는 몇몇 <a href="https://sourceware.org/glibc/wiki/MallocInternals#Switching_arenas" target="_blank">예외 상황</a>을 제외하면 스위치되지 않는다. ptmalloc2 은 이런 Arena 를 기본적으로 여러개 유지하기에 스레드들간 경합을 최소화할 수 있다.</p><p>또한 하나의 Arena 에서 각 스레드는 별도의 Heap 세그먼트를 유지하기 때문에 이러한 Heap 을 유지하는 freelist (Bin) 도 별도로 유지된다. 이것이 멀티 스레딩 환경에서 동시에 malloc 요청이 오더라도 대개 즉각적으로 (= 경합 없이) freelist 로부터 메모리를 할당해 줄 수 있는 이유이다.</p><blockquote class="prompt-note"><div><p><code class="language-plaintext highlighter-rouge">dlmalloc</code> 에서는 모든 스레드가 하나의 freelist 를 공유하기에 오직 하나의 스레드만이 임계 영역에 들어갈 수 있었다. 멀티스레딩 환경에서의 이런 경합을 개선하고자 나온것이 <code class="language-plaintext highlighter-rouge">dlmalloc</code> 을 포크한 <code class="language-plaintext highlighter-rouge">ptmalloc2</code> 이다.</p></div></blockquote><h2 id="arena-의-크기와-개수">Arena 의 크기와 개수 <a href="#arena-의-크기와-개수" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>Arena 크기와 개수는 어떻게 정해질까?</p><p><strong>Arena 의 최대 크기 = 64MiB</strong></p><div class="language-c highlighter-rouge"><div class="code-header"> <span data-label-text="C"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="cp"># 요즘은 대부분 64bit 시스템이므로 long == 8bytes 일 것이다.
</span>
<span class="n">define</span> <span class="n">DEFAULT_MMAP_THRESHOLD_MAX</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">long</span><span class="p">))</span>
<span class="n">define</span> <span class="n">HEAP_MAX_SIZE</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">DEFAULT_MMAP_THRESHOLD_MAX</span><span class="p">)</span>

<span class="cp"># "HEAP_MAX_SIZE" = 2 * (4 * 1024 * 1024 * 8)
# "HEAP_MAX_SIZE" = 67108864 "bit"
# "HEAP_MAX_SIZE" = 65536 "KiB"
# "HEAP_MAX_SIZE" = 64 "MiB"
</span></pre></table></code></div></div><p><strong>CPU 당 Arena 의 최대 개수 = 8</strong></p><div class="language-c highlighter-rouge"><div class="code-header"> <span data-label-text="C"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">define</span> <span class="n">NARENAS_FROM_NCORES</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">((</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="k">sizeof</span> <span class="p">(</span><span class="kt">long</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="o">?</span> <span class="mi">2</span> <span class="o">:</span> <span class="mi">8</span><span class="p">))</span>

<span class="cp"># long == 8bytes 이므로, CPU 당 ARENA 의 개수는 8 개다.
</span></pre></table></code></div></div><p>즉, 64bit 시스템에서 <strong><code class="language-plaintext highlighter-rouge">CPU 하나당 Arena 의 최대 크기는 512MiB</code></strong> (64MiB * 8) 이다.<br /> 우리 애플리케이션의 파드 cpu resource 는 10 이었다. 이는 Arena 의 개수가 최대 80개 까지 늘어날 수 있고 전체 크기는 최대 5GiB 이상 늘어날 수 있다는 이야기이다. 😱</p><details> <summary>잡담</summary> 여담이지만 cpu resource 리소스를 너무 많이 잡아 놓은 것 같다.<br /><br /> JVM 애플리케이션 특성상 앱 시작시 많은 CPU 자원을 소모하기 때문에 cpu 리소스를 많이 할당했지만 평시에는 cpu 자원을 거의 사용하지 않았다. 웜업 로직을 개선하든, 클러스터 버전업을 하면서 k8s 1.27 에 소개된 <a href="https://kubernetes.io/blog/2023/05/12/in-place-pod-resize-alpha/#java-processes-initialization-cpu-requirements">In-place Resource Resize</a> 라는 피쳐를 사용해보든 개선해야 할 사항이라 생각한다. </details><h1 id="ptmalloc2-의-메모리-파편화-문제">ptmalloc2 의 메모리 파편화 문제</h1><p>이제 위에서 언급한 malloc 의 메모리 파편화 이슈를 본격적으로 살펴본다.<br /> 이슈를 두가지 면에서 살펴보고, 테스트를 통해 내용을 검증해보자.</p><h2 id="point-1">Point 1. <a href="#point-1" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p><strong>Q.</strong></p><p><em>위에서 계산한 수치는 Arena 의 “최대” 크기일 뿐 <code class="language-plaintext highlighter-rouge">malloc()</code> 후에 <code class="language-plaintext highlighter-rouge">free()</code> 만 잘 되고 있다면 문제될게 없지 않은가?</em></p></blockquote><blockquote><p><strong>A.</strong></p><p>하지만 ptmalloc2 는 기대한 것처럼 동작하지 않는다. <code class="language-plaintext highlighter-rouge">free()</code> 가 호출되면 ptmalloc2 은 OS 로 (대체로) 메모리를 반환하지 않는다. 🤯 대신 반환할 메모리를 향후 있을 수도 있을 사용을 위해 freelist 로 만드는데, freelist 는 OS 관점에서는 애플리케이션이 여전히 사용중인(RSS) 메모리일 뿐이다.</p><p>좀 더 <a href="https://sourceware.org/glibc/wiki/MallocInternals#Free_Algorithm" target="_blank">복합적인 로직</a>이 있지만 단순화하면,</p><ul><li><code class="language-plaintext highlighter-rouge">free()</code> 호출 시, 향후 있을 사용을 위해 OS 로 메모리를 반환하지 않고 해당 <strong>Heap 을 freelist 로 할당</strong>한다.<li>이 말은 애플리케이션 메모리가 glibc 에 반환된 것이지, OS 에 반환된 것이 아니라는 말이다.<li>이같은 이유로 위에서 Chunk 를 설명할 때 Free Chunk 를 <strong>owned by glibc</strong> 로 표현했다.</ul></blockquote><p>👉 정리하면 <code class="language-plaintext highlighter-rouge">free()</code> 커맨드는 OS 로 메모리를 반환시키는 명령어라고 볼 수 없고, glibc 는 <code class="language-plaintext highlighter-rouge">free()</code> 된 메모리를 freelist 로 만드는데, 이는 여전히 애플리케이션에서 사용중인 (RSS) 메모리로 인식된다.</p><h2 id="point-2">Point 2. <a href="#point-2" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><blockquote><p><strong>Q.</strong></p><p><em>OS 로 메모리가 반환되지 않더라도 freelist 재활용만 잘하면 문제될게 없지 않은가?</em></p></blockquote><blockquote><p><strong>A.</strong></p><p>이 대목에서 glibc 의 메모리 점유와 관련된 버그 리포트들을 찾아볼 수 있다.<br /> 메모리 할당자가 기대처럼 freelist 재활용을 잘 하지 못하고, 새로운 Arena 를 생성하면서 메모리 점유량이 늘어난다는 내용이다.</p><ul><li><a href="https://github.com/cloudfoundry/java-buildpack/issues/320" target="_blank">https://github.com/cloudfoundry/java-buildpack/issues/320</a><li><a href="https://github.com/quarkusio/quarkus/issues/36204" target="_blank">https://github.com/quarkusio/quarkus/issues/36204</a></ul></blockquote><p>👉 요약하면 다음과 같은 일이 반복적으로 일어난다는 내용이다.</p><ol><li>메모리 요청이 들어온다.<li>재사용을 위해 남겨둔 freelist 를 사용하지 않고 또 다른 Arena 를 생성한다.<li>메모리를 모두 사용한 뒤 (향후있게 될 사용을 위해) OS 에 반환하지 않고 freelist 로 만든다.<li>Arena 에 사용되지 않는 freelist 들이 계속 쌓여간다.</ol><p><strong><code class="language-plaintext highlighter-rouge">2번</code></strong>: freelist 가 실제로 재활용되지 않는 경우가 많다며 버그리포트에서 지적하는 내용이다.<br /> <strong><code class="language-plaintext highlighter-rouge">3번</code></strong>: glibc 문서에서 설명하고 있는 ptmalloc2 의 스펙이다.<br /> <strong><code class="language-plaintext highlighter-rouge">4번</code></strong>: 실제로 일어나고 있다면 메모리 (RSS)가 계속 증가하는 주범이 될 것이다. (<strong>검증 필요</strong>)</p><h2 id="malloc_trim">malloc_trim() <a href="#malloc_trim" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>glibc 에 이런 freelist 를 실제 OS 로 반환하는 명령어로 <code class="language-plaintext highlighter-rouge">malloc_trim()</code> 명령어가 있다. 이는 외부에서 C-heap 을 trim 할 수 있도록 설계된 glibc 의 API 이다.</p><p>JVM 에 위 같은 문제가 있자, <code class="language-plaintext highlighter-rouge">malloc_trim()</code> 명령어를 애플리케이션 레벨(JDK)에서 호출할 수 있게 해달라는 요청이 있었다. 이는 JDK <code class="language-plaintext highlighter-rouge">11.0.18</code>, <code class="language-plaintext highlighter-rouge">17.0.2</code> 에 백포팅돼서 jcmd 명령어를 통해 freelist 를 정리할 수 있게 되었다.</p><p>👉 <code class="language-plaintext highlighter-rouge">jcmd &lt;pid&gt; System.trim_native_heap</code></p><ul><li><a href="https://bugs.openjdk.org/browse/JDK-8268893" target="_blank">https://bugs.openjdk.org/browse/JDK-8268893</a><li><a href="https://bugs.openjdk.org/browse/JDK-8273602" target="_blank">https://bugs.openjdk.org/browse/JDK-8273602</a></ul><p>또한 JVM 에서 이를 자동으로 호출하는 기능이 JDK <code class="language-plaintext highlighter-rouge">17.0.12</code>, <code class="language-plaintext highlighter-rouge">21.0.3</code> 부터 정식 기능으로 백포팅되어 <code class="language-plaintext highlighter-rouge">-XX:TrimNativeHeapInterval</code> 옵션을 통해 trim 주기를 설정할 수 있다. 초기에는 실험 기능으로 들어갔기 때문에 <code class="language-plaintext highlighter-rouge">-XX:+UnlockExperimentalVMOptions</code> 옵션을 함께 활성화 해야 했지만 지금은 공식 기능으로 채택되었다. 기본으로는 비활성화되어있고, 주기를 설정하면 활성화된다.</p><p><img width="1305" alt="스크린샷 2024-07-23 오후 4 34 45" data-src="https://github.com/user-attachments/assets/671354c7-0655-4f72-97fe-cd796f7fedf3" data-proofer-ignore></p><ul><li><a href="https://bugs.openjdk.org/browse/JDK-8293114" target="_blank">https://bugs.openjdk.org/browse/JDK-8293114</a><li><a href="https://bugs.openjdk.org/browse/JDK-8325496" target="_blank">https://bugs.openjdk.org/browse/JDK-8325496</a></ul><p><strong><em>Autotrim 기능은 JDK 11 엔 백포팅 해주지 않았다..😡 놓아줄 때가 된 듯 하다.</em></strong></p><h2 id="테스트--검증">테스트 &amp; 검증 <a href="#테스트--검증" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p>테스트를 통해서 위에서 언급한 <code class="language-plaintext highlighter-rouge">4번 (Arena 에 사용되지 않는 freelist 들이 계속 쌓여간다.)</code> 을 검증해보자.</p><p>메모리 할당 상태를 살펴보기 위해 pmap 커맨드를 이용해도 해당 메모리가 Arena 에 할당된 메모리인지 알기 힘든데, 다행히도 <a href="https://github.com/bric3/java-pmap-inspector" target="_blank">pmap 의 메모리 할당 상태를 분석해주는 java 애플리케이션</a>이 있다. inspector 를 사용하면 100% 는 아니지만 대략적으로 현재 메모리의 할당 상태를 확인해볼 수 있다. pmap 결과를 인자로 전달하면 inspector 가 메모리 할당 상태를 분석해준다.</p><p>분석 방법은 간단하다.</p><blockquote><ol><li><strong>운영 파드(pod) 중 하나의 라벨을 변경하여 서비스에서 제외시킨다.</strong><li><strong><code class="language-plaintext highlighter-rouge">jcmd &lt;pid&gt; System.trim_native_heap</code> 전 후 pmap 을 비교한다.</strong><li><strong>애플리케이션 RSS 수치 변동 여부를 확인한다.</strong></ol><p><strong>trim 전</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre>Memory mappings:
        JAVA_HEAP count=1     reserved=12058624   rss=11036196
          UNKNOWN count=168   reserved=960548     rss=728168
     MALLOC_ARENA count=259   reserved=16982044   rss=2221852
      JAVA_THREAD count=226   reserved=232328     rss=20364
 UNKNOWN_SEGMENT1 count=35    reserved=107660     rss=73952
 UNKNOWN_SEGMENT2 count=63    reserved=129024     rss=118456
  NON_JAVA_THREAD count=15    reserved=15480      rss=208
        CODE_HEAP count=3     reserved=245760     rss=179680
</pre></table></code></div></div><p><strong>trim 후</strong></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre>$ jattach 1 jcmd System.trim_native_heap
Connected to remote JVM
JVM response code = 0
Attempting trim...
Done.
Virtual size before: 30741876k, after: 30741748k, (-128k)
RSS before: 14379488k, after: 13552404k, (-827084k)               &lt;-- trimmed (단위: KB)
Swap before: Ok, after: Ok, (Ok)


Memory mappings:
        JAVA_HEAP count=1     reserved=12058624   rss=11035946
          UNKNOWN count=168   reserved=960548     rss=728168
     MALLOC_ARENA count=259   reserved=16982044   rss=1395884     &lt;-- about 800 MB trimmed from ARENA (단위: KiB)
      JAVA_THREAD count=226   reserved=232328     rss=20284
 UNKNOWN_SEGMENT1 count=35    reserved=107660     rss=73952
 UNKNOWN_SEGMENT2 count=63    reserved=129024     rss=118456
  NON_JAVA_THREAD count=15    reserved=15480      rss=208
        CODE_HEAP count=3     reserved=245760     rss=179692
</pre></table></code></div></div><blockquote class="prompt-warning"><div><p><code class="language-plaintext highlighter-rouge">MALLOC_ARENA</code> 개수가 259개, <code class="language-plaintext highlighter-rouge">reserved</code> 메모리가 16982044KiB 이다.<br /> 16982044 KiB / 259 == 65567 KiB == 64.03 MiB 로 정확히 아레나의 최대 크기이다.</p><p>계산식 대로면 애플리케이션 할당 cpu 가 10개이기 때문에 최대 Arena 는 80개여야 하는데..?<br /> 개수가 훨씬 많아서 이상하다가도, reserved 크기를 보면 아레나가 맞다.</p><p>이부분은 왜인지 아직 모르겠지만, 개수보다는 RSS 수치가 중요하기 때문에 우선 넘어간다.</p></div></blockquote><p><strong>애플리케이션 RSS 수치 확인</strong> <img width="808" alt="스크린샷 2024-07-23 오후 5 25 30" data-src="https://github.com/user-attachments/assets/5dd15ea2-a481-4c01-9c24-6f80d0cf3011" data-proofer-ignore> <em>수일간 점진적으로 증가해오던 메모리 수치가 trim 후, 훅 떨어졌다.</em></p><p>앞서 설명했듯이 JDK 17, 21 을 사용하면 이것을 일정주기로 auto trim 해줄 수 있다. JDK 11 에선 jcmd 명령어를 주기적으로 실행해주는 스크립트를 작성해도 문제가 없을 것 같지만 이 참에 JDK 버전을 올리는 방향이 더 좋아보인다.</p></blockquote><p>이로써 실제 trim 을 통해 <code class="language-plaintext highlighter-rouge">MALLOC_ARENA</code> 에서 대략 800MB 가량의 메모리가 OS 로 반환되었음을 확인했다. 이말은 JVM <code class="language-plaintext highlighter-rouge">pid == 1</code> 에 820MB 가량의 메모리가 freelist (사용되지 않는 상태) 상태로 남아있었다는 말이다.</p><p>또한 애플리케이션의 에이징됨에 따라 정리되는 (trimmed) 메모리가 커지는 것을 관찰할 수 있었는데 이를 통해 freelist 가 시간이 지남에 따라 축적된다는 점을 알 수 있었고, 위에서 말한 <strong><code class="language-plaintext highlighter-rouge">4번 (Arena 에 사용되지 않는 Freelist 들이 계속 쌓여간다.)</code>을 경험적으로 검증</strong>한 셈이다.</p><h1 id="정리">정리</h1><ul><li><strong>현상</strong>: JVM 수준에서 관측할 수 없는 Native 메모리 사용량의 지속적인 증가 현상<li><strong>원인</strong>: glibc 를 표준 C 라이브러리로 사용하는 리눅스 시스템 C 메모리 할당자 (= ptmalloc2) 의 메모리 파편화 문제</ul><h2 id="개선-방안">개선 방안 <a href="#개선-방안" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><p><strong>👉 JDK <code class="language-plaintext highlighter-rouge">17.0.12</code>, <code class="language-plaintext highlighter-rouge">21.0.3</code> 이상 버전</strong></p><ul><li><code class="language-plaintext highlighter-rouge">-XX:TrimNativeHeapInterval</code> 옵션을 활성화 하여 JVM 이 파편화된 freelist 메모리를 주기적으로 정리하도록 한다.</ul><p><strong>👉 그 이전 버전</strong></p><ul><li><p>JDK <code class="language-plaintext highlighter-rouge">17.0.12</code>, <code class="language-plaintext highlighter-rouge">21.0.3</code> 이상 버전으로 업그레이드한다. 😄</p><li><p>jcmd 의 <code class="language-plaintext highlighter-rouge">System.trim_native_heap</code> 를 호출하는 스크립트를 일정 주기로 실행시킨다.<br /> JDK <code class="language-plaintext highlighter-rouge">11.0.18</code>, <code class="language-plaintext highlighter-rouge">17.0.2</code> 이상의 jcmd 에만 포함된 기능이다.</p><li><p>시스템의 메모리 할당자를 변경한다. (jemalloc, tcmalloc)<br /> 이런(사용해본적 없는 메모리 할당자를 사용하는) 부담을 가지고 갈바엔 JDK 업데이트를 하는게 나을 것 같다.</p><li><p>시스템 최대 ARENA 개수(<code class="language-plaintext highlighter-rouge">MALLOC_ARENA_MAX</code>)를 조정한다.<br /> C Heap 을 제한하는 방식이라 퍼포먼스 이슈가 있을 수도 있다.<br /> 이 방식을 택하려면 상황을 잘 고려해서 사용해야 할 것 같다.</p></ul><h2 id="note">Note. <a href="#note" class="anchor"><i class="fas fa-hashtag"></i></a></h2></h2><ol><li><p><strong>이론상 CPU 가 하나 추가될 때마다 Native 메모리가 512MiB 까지 더 필요할 수도 있다는 점</strong><br /> 사실 ptmalloc2 이 항상 freelist 를 재활용하지 않는 것도 아니고, OS 로 메모리 반환을 항상 하지 않는 것도 아니다. 때문에 메모리 점유율은 증가하긴 하지만 완만하고 느리다.<br /> 배포 주기가 매우 긴 조직이 아니면 모든 Arena 가 이론상의 최대 수치 만큼 도달하지는 않을 것 같다. 중요한 점은 문제를 인지하고 이에 대한 대비를 하느냐의 여부일 것 같다.</p><li><p><strong>ptmalloc2 는 (JVM 을 구동할 때?) 메모리 파편화 <del>이슈</del>가 있다는 점</strong><br /> 사실은 ptmalloc2 자체의 “이슈”라기 보다는 ptmalloc2 의 메모리 할당 알고리즘이 JVM 과 핏하지 않은 것 뿐이라는 생각이 더 크다.</p><li><p><strong>JVM <code class="language-plaintext highlighter-rouge">17.0.9</code>, <code class="language-plaintext highlighter-rouge">21.0.1</code> 이상 버전에서는 <code class="language-plaintext highlighter-rouge">-XX:TrimNativeHeapInterval</code> 을 세팅하는게 좋다는 점</strong><br /> 위 세팅은 기본적으로 disable 되어있다. Native 메모리 파편화를 방지하기 위해서 세팅해두면 좋을 것 같다.</p></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/jvm/'>JVM</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/jdk/" class="post-tag no-text-decoration" >JDK</a> <a href="/tags/jvm/" class="post-tag no-text-decoration" >JVM</a> <a href="/tags/leak/" class="post-tag no-text-decoration" >leak</a> <a href="/tags/c/" class="post-tag no-text-decoration" >C</a> <a href="/tags/glibc/" class="post-tag no-text-decoration" >glibc</a> <a href="/tags/malloc/" class="post-tag no-text-decoration" >malloc</a> <a href="/tags/jcmd/" class="post-tag no-text-decoration" >jcmd</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc - 작심삼일 블로그&amp;url=https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc - 작심삼일 블로그&amp;u=https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https://leeby-devel.github.io/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/&amp;text=JVM 메모리 누수 &gt; Native 메모리 &gt; glibc malloc - 작심삼일 블로그" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="leeby-devel/leeby-devel.github.io" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/JEP346-unused-committed-memory-%EB%A5%BC-os-%EB%A1%9C-%EB%B0%98%ED%99%98/">JEP 346 > unused (미사용) committed memory 를 OS 로 반환 (feat. 인프라 비용 절감)</a><li><a href="/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Native-%EB%A9%94%EB%AA%A8%EB%A6%AC-glibc-malloc/">JVM 메모리 누수 > Native 메모리 > glibc malloc</a><li><a href="/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Heap-lettuce/">JVM 메모리 누수 > Heap > lettuce</a><li><a href="/posts/k8s-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%8C%A8%ED%82%B7-%EC%BA%A1%EC%B3%90%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95/">k8s 환경에서 패킷 캡쳐하는 방법 > tcpdump, wireshark</a><li><a href="/posts/JVM-Container-Awareness-Bug-cgroup-v2-JDK-11.0.16/">JVM Container Awareness Bug (cgroup v2) > JDK 11.0.16</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/jdk/">JDK</a> <a class="post-tag" href="/tags/jvm/">JVM</a> <a class="post-tag" href="/tags/jcmd/">jcmd</a> <a class="post-tag" href="/tags/leak/">leak</a> <a class="post-tag" href="/tags/c/">C</a> <a class="post-tag" href="/tags/cgroup/">cgroup</a> <a class="post-tag" href="/tags/container-awareness/">Container Awareness</a> <a class="post-tag" href="/tags/dispatcherservlet/">DispatcherServlet</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/g1gc/">G1GC</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Heap-lettuce/"><div class="card-body"> <em class="timeago small" data-ts="1712923200" > 2024-04-12 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>JVM 메모리 누수 > Heap > lettuce</h3><div class="text-muted small"><p> 최근 회사 애플리케이션의 JVM 메모리 누수를 찾아낸 뒤 픽스했다. 과정을 정리해두면 메모리 누수를 또 찾고 있을 미래의 나에게도 도움이 될 것 같아 정리해본다. 트러블 슈팅 배경 애플리케이션 파드가 일정 시간 이상 에이징이 되면 RESTART 되고 있다. 사내 메트릭 분석 툴에 힙 히스토그램을 시계열로 분석할 수 있는 기능이 있어 이를 활용해봤다...</p></div></div></a></div><div class="card"> <a href="/posts/JVM-Container-Awareness-Bug-cgroup-v2-JDK-11.0.16/"><div class="card-body"> <em class="timeago small" data-ts="1667634840" > 2022-11-05 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>JVM Container Awareness Bug (cgroup v2) > JDK 11.0.16</h3><div class="text-muted small"><p> 저처럼 글 읽기를 싫어하시는 분들을 위한 결론 Kubernetes 환경에서 JDK 11 을 사용중이라면 JDK 11.0.16 이상의 버전 사용을 권장한다. JDK 12 - 14 버전을 사용중이라면 15 이상의 버전 사용을 권장한다. 그렇지 않으면 소중한 Pod 가 OOMKilled 당할 수 있다. 특히 Init...</p></div></div></a></div><div class="card"> <a href="/posts/JEP346-unused-committed-memory-%EB%A5%BC-os-%EB%A1%9C-%EB%B0%98%ED%99%98/"><div class="card-body"> <em class="timeago small" data-ts="1731242340" > 2024-11-10 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>JEP 346 > unused (미사용) committed memory 를 OS 로 반환 (feat. 인프라 비용 절감)</h3><div class="text-muted small"><p> 들어가며… 최근 회사에서 JDK, Spring Boot, Kotlin, Gradle 등 기반 시스템 버전 업그레이드하는 작업을 주도했다. 그 중 JDK 버전은 11 → 17, 17 → 21 두 단계로 진행할 계획이었고, 계획대로 JDK 17 부터 버전 업데이트를 했다. 업데이트 후 애플리케이션 메트릭을 살펴보는데 Heap 메모리 풋프린트가 이전과 다른...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/JVM-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EB%88%84%EC%88%98-Heap-lettuce/" class="btn btn-outline-primary" prompt="Older"><p>JVM 메모리 누수 > Heap > lettuce</p></a> <a href="/posts/k8s-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%8C%A8%ED%82%B7-%EC%BA%A1%EC%B3%90%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95/" class="btn btn-outline-primary" prompt="Newer"><p>k8s 환경에서 패킷 캡쳐하는 방법 > tcpdump, wireshark</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://twitter.com/username">leeby</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/jdk/">JDK</a> <a class="post-tag" href="/tags/jvm/">JVM</a> <a class="post-tag" href="/tags/jcmd/">jcmd</a> <a class="post-tag" href="/tags/leak/">leak</a> <a class="post-tag" href="/tags/c/">C</a> <a class="post-tag" href="/tags/cgroup/">cgroup</a> <a class="post-tag" href="/tags/container-awareness/">Container Awareness</a> <a class="post-tag" href="/tags/dispatcherservlet/">DispatcherServlet</a> <a class="post-tag" href="/tags/docker/">docker</a> <a class="post-tag" href="/tags/g1gc/">G1GC</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-FM1HTHQVZ8"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-FM1HTHQVZ8'); }); </script>
